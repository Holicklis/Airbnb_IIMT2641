cat("Root Mean Squared Error (RMSE):", rmse_lm, "\n")
cat("R-squared:", rsquared_lm, "\n")
cat("Normalized RMSE:", normalized_rmse_lm, "\n")
# Train the random forest model
library(randomForest)
rf_model <- randomForest(price ~ ., data = training_data, ntree = 100, importance = TRUE)
#rf_model <- randomForest(price ~ ., data = training_data)summa
# Make predictions on the testing dataset
testing_data$predicted_price <- predict(rf_model, testing_data)
# Train the random forest model
library(randomForest)
rf_model <- randomForest(price ~ ., data = training_data, ntree = 100, importance = TRUE)
#rf_model <- randomForest(price ~ ., data = training_data)summa
# Make predictions on the testing dataset
testing_data$predicted_price <- predict(rf_model, testing_data)
#SSE
SSE_rf <- sum((testing_data$predicted_price - testing_data$price)^2)
SSE_rf
#SST
SST_rf <- sum((testing_data$price - mean(testing_data$price))^2)
SST_rf
#R2
R2_rf <- 1 - SSE_rf/SST_rf
R2_rf
importance(rf_model)
#SSE
SSE_rf <- sum((testing_data$predicted_price - testing_data$price)^2)
SSE_rf
#SST
SST_rf <- sum((testing_data$price - mean(testing_data$price))^2)
SST_rf
#R2
R2_rf <- 1 - SSE_rf/SST_rf
R2_rf
importance(rf_model)
#SSE
SSE_rf <- sum((testing_data$predicted_price - testing_data$price)^2)
SSE_rf
#SST
SST_rf <- sum((testing_data$price - mean(testing_data$price))^2)
SST_rf
#R2
R2_rf <- 1 - SSE_rf/SST_rf
R2_rf
importance(rf_model)
# Calculate the performance metrics
mae_rf <- mean(abs(testing_data$predicted_price - testing_data$price))
mse_rf <- mean((testing_data$predicted_price - testing_data$price)^2)
rmse_rf <- sqrt(mse_rf)
rsquared_rf <- R2_rf
#normailzed rmse in range 0-1
#normalized RMSE max price-min price
normalized_rmse_rf <- rmse_rf/(max(testing_data$price)-min(testing_data$price))
# Print the performance metrics
cat("Mean Absolute Error (MAE):", mae_rf, "\n")
cat("Mean Squared Error (MSE):", mse_rf, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_rf, "\n")
cat("R-squared:", rsquared_rf, "\n")
cat("Normalized RMSE:", normalized_rmse_rf, "\n")
# Make predictions for the first 5 records in the testing dataset
all_test_data <-testing_data
all_test_data$predicted_price <- predict(rf_model, all_test_data)
subset_test_data <- testing_data[1:5, ]
subset_test_data$predicted_price <- predict(rf_model, subset_test_data)
subset_test_data
importance_table <- importance(rf_model)
sorted_importance_table <- as.data.frame(importance_table)
#rename
colnames(sorted_importance_table) <- c("importance")
names(sorted_importance_table)[is.na(names(sorted_importance_table))] <- "NA_Name"
names(sorted_importance_table)[names(sorted_importance_table) == ""] <- "Empty_Name"
# order by importance
sorted_importance_table <- sorted_importance_table %>% arrange(desc(importance))
# Display the sorted importance table
sorted_importance_table$Variable <- row.names(sorted_importance_table)
sorted_importance_table
ggplot(sorted_importance_table, aes(x = reorder(Variable, importance), y = importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
theme_minimal() +
labs(x = "Variable",
y = "IncNodePurity",
title = "Variable Importance in Random Forest Model") +
theme(plot.title = element_text(hjust = 0.5))
library(dplyr)
library(ggplot2)
library(readr)
library(HistData)
library(stringr)
library(tidyverse)
library(maps)
library(sf)
library(mapview)
library(ggrepel)
library(cartogram)
# Read datasets
data_march <- read_csv("airbnb_data_march_2022.csv")
data_june <- read_csv("airbnb_data_june_2022.csv")
data_september <- read_csv("airbnb_data_september_2022.csv")
data_december <- read_csv("airbnb_data_december_2022.csv")
data_june_2023 <- read_csv("airbnb_data_june_2023.csv")
data_september_2023 <- read_csv("airbnb_data_september_2023.csv")
data_december_2023 <- read_csv("airbnb_data_december_2023.csv")
common_columns <- base::intersect(base::intersect(base::intersect(base::intersect(base::intersect(base::intersect(names(data_march), names(data_june)), names(data_september)), names(data_december)), names(data_june_2023)), names(data_september_2023)), names(data_december_2023))
common_columns<-intersect(intersect(intersect(intersect(intersect(intersect(names(data_march), names(data_june)), names(data_september)), names(data_december)), names(data_june_2023)), names(data_september_2023)), names(data_december_2023))
data_march_common <- data_march %>% select(all_of(common_columns))
data_june_common <- data_june %>% select(all_of(common_columns))
data_september_common <- data_september %>% select(all_of(common_columns))
data_december_common <- data_december %>% select(all_of(common_columns))
data_june_2023_common <- data_june_2023 %>% select(all_of(common_columns))
data_september_2023_common <- data_september_2023 %>% select(all_of(common_columns))
data_december_2023_common <- data_december_2023 %>% select(all_of(common_columns))
combined_data <- rbind(data_march_common, data_june_common, data_september_common, data_december_common, data_june_2023_common, data_september_2023_common, data_december_2023_common)
# combined_data
head(combined_data)
ncol(combined_data)
# Set the threshold for missing values
threshold <- 0.7
# Calculate the percentage of missing values for each column
missing_percentage <- combined_data %>% summarise_all(funs(sum(is.na(.))/n()))
# Identify columns with less than 70% missing values
columns_to_keep <- names(missing_percentage)[apply(missing_percentage, 2, function(x) x < threshold)]
# Keep only the selected columns
cleaned_data <- combined_data %>% select(all_of(columns_to_keep))
cleaned_data
ncol(cleaned_data)
property_type_counts <- cleaned_data %>%
group_by(property_type) %>%
summarise(count = n()) %>%
arrange(desc(count))
property_type_counts
### Frequency bar chart
#ggplot(property_type_counts) +
#geom_col(aes(y=property_type, x=count)) +
#theme()
#ggtitle("Distribution of properties based on its type")
# Select the top 5 property types
top_5_property_types <- property_type_counts %>%
top_n(5, count) %>%
pull(property_type)
# Filter the dataset to include only the top 5 property types
filtered_data <- cleaned_data %>%
filter(property_type %in% top_5_property_types)
filtered_data
# Convert price column to numeric
filtered_data$price <- as.numeric(gsub("[^0-9.]", "", filtered_data$price))
filtered_data
# Remove rows with missing price values
filtered_data <- filtered_data %>% drop_na(price)
filtered_data
highestrow <- filtered_data %>% arrange(desc(price)) %>% head(10)
highestrow
# Remove rows with price with top 2% highest values and bottom 2% lowest values
filtered_data <- filtered_data %>%
filter(price > quantile(price, 0.02) & price < quantile(price, 0.98))
filtered_data
ggplot(filtered_data, aes(neighbourhood_cleansed, fill = property_type)) +
geom_histogram(stat = "count", position = 'fill') +
theme_minimal(base_size = 13)+ xlab("") + ylab("") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Proportion of Property Type in Each District") +
scale_fill_discrete(name = "Property Type")
district_data <- filtered_data |> group_by(neighbourhood_cleansed) |> summarise(price = round(mean(price), 2))
ggplot(filtered_data, aes(price)) +
geom_histogram(bins = 30, aes(y = ..density..), fill = "steelblue") +
geom_density(alpha = 0.6, fill = "black", color = FALSE) +
geom_text(data = district_data, y = 0.003, aes(x = 2000, label = paste("Mean = $", price)), color = "black", size = 3) +
facet_wrap( ~ neighbourhood_cleansed) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_color_discrete(name = "Property Type") +
labs(title = "Distribution of Price by Neighbourhood")
filtered_data |> ggplot(aes(beds, price, color = property_type, size = accommodates, alpha = 0.05)) +
geom_point(na.rm = TRUE) +
ggtitle("Number of beds vs. price, by Property Type & Accommodation Capacity") +
xlab("# of Beds") +
ylab("Price (HKD)") +
scale_color_discrete(name = "Property Type") +
guides(alpha = FALSE) +
scale_size_continuous(name = "Accommodation Capacity") +
theme_classic()
# Subset numeric columns
num_var <- filtered_data[, sapply(filtered_data, is.numeric)]
# Subset remaining (categorical) columns
cat_var <- filtered_data[, !(names(filtered_data) %in% names(num_var))]
# Drop unique identifier columns from subsets
num_var <- subset(num_var, select = c(accommodates, bedrooms, beds, price, review_scores_rating, reviews_per_month))
cat_var <- subset(cat_var, select = c(host_response_time, host_response_rate, host_acceptance_rate, host_is_superhost, host_verifications, host_identity_verified))
# Get summary stats for numerical variables
summary(num_var)
col.sd <- apply(na.omit(num_var), 2, sd)
col.sd
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(readr)
library(HistData)
library(stringr)
library(tidyverse)
library(maps)
library(sf)
library(mapview)
library(ggrepel)
library(cartogram)
# Read datasets
data_march <- read_csv("airbnb_data_march_2022.csv")
data_june <- read_csv("airbnb_data_june_2022.csv")
data_september <- read_csv("airbnb_data_september_2022.csv")
data_december <- read_csv("airbnb_data_december_2022.csv")
data_june_2023 <- read_csv("airbnb_data_june_2023.csv")
data_september_2023 <- read_csv("airbnb_data_september_2023.csv")
data_december_2023 <- read_csv("airbnb_data_december_2023.csv")
common_columns <- base::intersect(base::intersect(base::intersect(base::intersect(base::intersect(base::intersect(names(data_march), names(data_june)), names(data_september)), names(data_december)), names(data_june_2023)), names(data_september_2023)), names(data_december_2023))
common_columns<-intersect(intersect(intersect(intersect(intersect(intersect(names(data_march), names(data_june)), names(data_september)), names(data_december)), names(data_june_2023)), names(data_september_2023)), names(data_december_2023))
data_march_common <- data_march %>% select(all_of(common_columns))
data_june_common <- data_june %>% select(all_of(common_columns))
data_september_common <- data_september %>% select(all_of(common_columns))
data_december_common <- data_december %>% select(all_of(common_columns))
data_june_2023_common <- data_june_2023 %>% select(all_of(common_columns))
data_september_2023_common <- data_september_2023 %>% select(all_of(common_columns))
data_december_2023_common <- data_december_2023 %>% select(all_of(common_columns))
combined_data <- rbind(data_march_common, data_june_common, data_september_common, data_december_common, data_june_2023_common, data_september_2023_common, data_december_2023_common)
# combined_data
head(combined_data)
ncol(combined_data)
# Set the threshold for missing values
threshold <- 0.7
# Calculate the percentage of missing values for each column
missing_percentage <- combined_data %>% summarise_all(funs(sum(is.na(.))/n()))
# Identify columns with less than 70% missing values
columns_to_keep <- names(missing_percentage)[apply(missing_percentage, 2, function(x) x < threshold)]
# Keep only the selected columns
cleaned_data <- combined_data %>% select(all_of(columns_to_keep))
cleaned_data
ncol(cleaned_data)
property_type_counts <- cleaned_data %>%
group_by(property_type) %>%
summarise(count = n()) %>%
arrange(desc(count))
property_type_counts
### Frequency bar chart
#ggplot(property_type_counts) +
#geom_col(aes(y=property_type, x=count)) +
#theme()
#ggtitle("Distribution of properties based on its type")
# Select the top 5 property types
top_5_property_types <- property_type_counts %>%
top_n(5, count) %>%
pull(property_type)
# Filter the dataset to include only the top 5 property types
filtered_data <- cleaned_data %>%
filter(property_type %in% top_5_property_types)
filtered_data
# Convert price column to numeric
filtered_data$price <- as.numeric(gsub("[^0-9.]", "", filtered_data$price))
filtered_data
# Remove rows with missing price values
filtered_data <- filtered_data %>% drop_na(price)
filtered_data
highestrow <- filtered_data %>% arrange(desc(price)) %>% head(10)
highestrow
# Remove rows with price with top 2% highest values and bottom 2% lowest values
filtered_data <- filtered_data %>%
filter(price > quantile(price, 0.02) & price < quantile(price, 0.98))
filtered_data
ggplot(filtered_data, aes(neighbourhood_cleansed, fill = property_type)) +
geom_histogram(stat = "count", position = 'fill') +
theme_minimal(base_size = 13)+ xlab("") + ylab("") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Proportion of Property Type in Each District") +
scale_fill_discrete(name = "Property Type")
district_data <- filtered_data |> group_by(neighbourhood_cleansed) |> summarise(price = round(mean(price), 2))
ggplot(filtered_data, aes(price)) +
geom_histogram(bins = 30, aes(y = ..density..), fill = "steelblue") +
geom_density(alpha = 0.6, fill = "black", color = FALSE) +
geom_text(data = district_data, y = 0.003, aes(x = 2000, label = paste("Mean = $", price)), color = "black", size = 3) +
facet_wrap( ~ neighbourhood_cleansed) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_color_discrete(name = "Property Type") +
labs(title = "Distribution of Price by Neighbourhood")
filtered_data |> ggplot(aes(beds, price, color = property_type, size = accommodates, alpha = 0.05)) +
geom_point(na.rm = TRUE) +
ggtitle("Number of beds vs. price, by Property Type & Accommodation Capacity") +
xlab("# of Beds") +
ylab("Price (HKD)") +
scale_color_discrete(name = "Property Type") +
guides(alpha = FALSE) +
scale_size_continuous(name = "Accommodation Capacity") +
theme_classic()
# Subset numeric columns
num_var <- filtered_data[, sapply(filtered_data, is.numeric)]
# Subset remaining (categorical) columns
cat_var <- filtered_data[, !(names(filtered_data) %in% names(num_var))]
# Drop unique identifier columns from subsets
num_var <- subset(num_var, select = c(accommodates, bedrooms, beds, price, review_scores_rating, reviews_per_month))
cat_var <- subset(cat_var, select = c(host_response_time, host_response_rate, host_acceptance_rate, host_is_superhost, host_verifications, host_identity_verified))
# Get summary stats for numerical variables
summary(num_var)
col.sd <- apply(na.omit(num_var), 2, sd)
col.sd
# location
ggplot(filtered_data, aes(x = review_scores_location, y = review_scores_rating, color=price)) +
geom_jitter(size = 0.4, alpha = 0.2, na.rm = TRUE)
# accuracy
ggplot(filtered_data, aes(x = review_scores_accuracy, y = review_scores_rating, color=price)) +
geom_jitter(size = 0.4, alpha = 0.2, na.rm = TRUE)
# cleanliness
ggplot(filtered_data, aes(x = review_scores_cleanliness, y = review_scores_rating, color=price)) +
geom_jitter(size = 0.4, alpha = 0.2, na.rm = TRUE)
# location
ggplot(filtered_data, aes(x = review_scores_location, y = review_scores_rating, color=price)) +
geom_jitter(size = 0.4, alpha = 0.2, na.rm = TRUE)
# accuracy
ggplot(filtered_data, aes(x = review_scores_accuracy, y = review_scores_rating, color=price)) +
geom_jitter(size = 0.4, alpha = 0.2, na.rm = TRUE)
# cleanliness
ggplot(filtered_data, aes(x = review_scores_cleanliness, y = review_scores_rating, color=price)) +
geom_jitter(size = 0.4, alpha = 0.2, na.rm = TRUE)
filtered_data
# Extract unique amenities
amenities_freq <- filtered_data %>%
pull(amenities) %>%
str_extract_all('(?<=\")\\w[^,"]+') %>%
unlist() %>%
table() %>%
as.data.frame() %>%
arrange(desc(Freq))
amenities_freq
# Calculate the 10% threshold
threshold <- nrow(filtered_data) * 0.10
# Select amenities that appear in at least 10% of the listings
selected_amenities <- amenities_freq %>%
filter(Freq >= threshold) %>% select(1)
### amenities frequency chart
top0.1_amenities <- amenities_freq |>  filter(Freq >= threshold)
ggplot(top0.1_amenities, aes(x = ., y = Freq)) +
geom_col() +
geom_text_repel(aes(label = Freq)) +
ggtitle("Frequency of amenities") +
xlab("Amenities") +
ylab("Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
selected_amenities[] <- data.frame(lapply(selected_amenities, as.character))
selected_amenities= as.list(selected_amenities)
final_data <- filtered_data
selected_amenities<-selected_amenities[1]
selected_amenities<-unlist(selected_amenities)
class(selected_amenities)
for (amenity in selected_amenities) {
final_data <- final_data %>%
mutate(!!amenity := ifelse(str_detect(amenities, amenity), 1, 0))
}
final_data <- final_data %>%
select(-amenities)
head(final_data)
class(selected_amenities)
### Before & After conversion for amenities
filtered_data |> select(id,amenities) |> slice(1)
final_data |>  select(id, Wifi:"Shower gel") |> slice(1)
selected_amenities <- gsub("[-. *()]", "_", selected_amenities)
colnames(final_data) <- gsub("[-. *()]", "_", colnames(final_data))
selected_amenities
colnames(final_data)
final_data_reduced <- final_data %>% select(-neighbourhood) %>% select(-latitude) %>% select(-longitude)
final_data_reduced <- final_data_reduced %>% select(-host_thumbnail_url) %>% select(-host_picture_url)
final_data_reduced <- final_data_reduced %>% select(-scrape_id)%>% select(-picture_url)
head(final_data_reduced)
colnames(final_data_reduced)
selected_features <- c("host_response_time", "host_acceptance_rate", "host_is_superhost", "host_listings_count", "host_identity_verified", "neighbourhood_cleansed", "property_type", "room_type", "accommodates", "bedrooms", "beds", "price", "number_of_reviews", "review_scores_rating","minimum_nights_avg_ntm", "maximum_nights_avg_ntm")
selected_features_old<- c("host_acceptance_rate", "host_is_superhost", "host_identity_verified", "neighbourhood_cleansed", "property_type", "room_type", "accommodates", "bedrooms", "beds", "price", "number_of_reviews", "review_scores_rating")
all_features <- c(selected_features, selected_amenities)
all_features
#all_features <- selected_features
library(randomForest)
library(caret)
set.seed(42)
# Create a partition of the data
final_data_selected <- final_data_reduced[all_features]
final_data_selected$host_acceptance_rate <- as.numeric(gsub("[^0-9.]", "", final_data_selected$host_acceptance_rate))
final_data_selected$host_acceptance_rate <- final_data_selected$host_acceptance_rate/100
final_data_selected
#convert all character to factor
final_data_selected <- final_data_selected %>% mutate_if(is.character, as.factor)
#replace all na in logical variable with false, all column
final_data_selected <- final_data_selected %>% mutate_if(is.logical, ~replace(., is.na(.), FALSE))
final_data_imputed <- final_data_selected
#impute missing value with median
final_data_imputed <- final_data_imputed %>% mutate_if(is.numeric, ~replace(., is.na(.), median(., na.rm = TRUE)))
#drop na
final_data_imputed <- final_data_imputed %>% drop_na()
final_data_imputed
partition <- caret::createDataPartition(y = final_data_imputed$price, p = 0.80, list = FALSE)
training_data <- final_data_imputed[partition, all_features]
testing_data <- final_data_imputed[-partition, all_features]
testing_data_laso <- testing_data
training_data_lm <- training_data
testing_data_lm <- testing_data
lm1 <- lm(price ~., data = training_data_lm)
summary(lm1)
training_data_reduced_lm <- select(training_data_lm, -"room_type")
lm2 <- lm(price ~ ., data = training_data_reduced_lm)
# summary(lm2)
library(car)
vif(lm2)
testing_data_lm$predicted_price <- predict(lm1, testing_data_lm)
testing_data_reduced_lm <- select(testing_data, -"room_type")
testing_data_reduced_lm$predicted_price <- predict(lm2, testing_data_lm)
#SSE
SSE_lm <- sum((testing_data_lm$predicted_price - testing_data_lm$price)^2)
SSE_lm
#SST
SST_lm <- sum((testing_data_lm$price - mean(testing_data_lm$price))^2)
SST_lm
#R2
R2_lm <- 1 - SSE/SST
training_data_lm <- training_data
testing_data_lm <- testing_data
lm1 <- lm(price ~., data = training_data_lm)
summary(lm1)
training_data_reduced_lm <- select(training_data_lm, -"room_type")
lm2 <- lm(price ~ ., data = training_data_reduced_lm)
# summary(lm2)
library(car)
vif(lm2)
testing_data_lm$predicted_price <- predict(lm1, testing_data_lm)
testing_data_reduced_lm <- select(testing_data, -"room_type")
testing_data_reduced_lm$predicted_price <- predict(lm2, testing_data_lm)
#SSE
SSE_lm <- sum((testing_data_lm$predicted_price - testing_data_lm$price)^2)
SSE_lm
#SST
SST_lm <- sum((testing_data_lm$price - mean(testing_data_lm$price))^2)
SST_lm
#R2
R2_lm <- 1 - SSE_lm/SST_lm
R2_lm
#SSE
paste("for reduced testing data:")
SSE_lm2 <- sum((testing_data_reduced_lm$predicted_price - testing_data_reduced_lm$price)^2)
SSE_lm2
#SST
SST_lm2 <- sum((testing_data_reduced_lm$price - mean(testing_data_reduced_lm$price))^2)
SST_lm2
#R2
R2_lm2 <- 1 - SSE_lm2/SST_lm2
R2_lm2
# Calculate the performance metrics
mae_lm <- mean(abs(testing_data_lm$predicted_price - testing_data_lm$price))
mse_lm <- mean((testing_data_lm$predicted_price - testing_data_lm$price)^2)
rmse_lm <- sqrt(mse_lm)
rsquared_lm <- R2_lm2
#normailzed rmse in range 0-1
#normalized RMSE max price-min price
normalized_rmse_lm <- rmse_lm/(max(testing_data_lm$price)-min(testing_data_lm$price))
# Print the performance metrics
cat("Mean Absolute Error (MAE):", mae_lm, "\n")
cat("Mean Squared Error (MSE):", mse_lm, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_lm, "\n")
cat("R-squared:", rsquared_lm, "\n")
cat("Normalized RMSE:", normalized_rmse_lm, "\n")
# Train the random forest model
library(randomForest)
rf_model <- randomForest(price ~ ., data = training_data, ntree = 100, importance = TRUE)
#rf_model <- randomForest(price ~ ., data = training_data)summa
# Make predictions on the testing dataset
testing_data$predicted_price <- predict(rf_model, testing_data)
#SSE
SSE_rf <- sum((testing_data$predicted_price - testing_data$price)^2)
SSE_rf
#SST
SST_rf <- sum((testing_data$price - mean(testing_data$price))^2)
SST_rf
#R2
R2_rf <- 1 - SSE_rf/SST_rf
R2_rf
importance(rf_model)
#SSE
SSE_rf <- sum((testing_data$predicted_price - testing_data$price)^2)
SSE_rf
#SST
SST_rf <- sum((testing_data$price - mean(testing_data$price))^2)
SST_rf
#R2
R2_rf <- 1 - SSE_rf/SST_rf
R2_rf
importance(rf_model)
# Calculate the performance metrics
mae_rf <- mean(abs(testing_data$predicted_price - testing_data$price))
mse_rf <- mean((testing_data$predicted_price - testing_data$price)^2)
rmse_rf <- sqrt(mse_rf)
rsquared_rf <- R2_rf
#normailzed rmse in range 0-1
#normalized RMSE max price-min price
normalized_rmse_rf <- rmse_rf/(max(testing_data$price)-min(testing_data$price))
# Print the performance metrics
cat("Mean Absolute Error (MAE):", mae_rf, "\n")
cat("Mean Squared Error (MSE):", mse_rf, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_rf, "\n")
cat("R-squared:", rsquared_rf, "\n")
cat("Normalized RMSE:", normalized_rmse_rf, "\n")
cforest(rf_model)
#plot the random forest tree
library(reprtree)
packages.install("reprtree")
packages.install("reprtree")
install.packages("reprtree")
#plot the random forest tree
library(reprtree)
#plot the random forest tree
library(reprtree)
#plot the random forest tree
getTree(rf_model, k = 2)
View(final_data_selected)
